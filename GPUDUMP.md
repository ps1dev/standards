# GPU Dump file format

This document describes the common format of GPU dump files generated by various tools. The GPU dump file format is designed to provide a structured representation of the commands sent to the GPU during a timespan, which can be useful for debugging and performance analysis, and can be used to replay a sequence of GPU commands either in emulators or the real hardware. The source code for replaying GPU commands is outside the scope of this document, but is a straightforward task once the dump format is understood. There can be other applications for the GPU dump file format, such as live previewing of a scene in a graphics application.

The GPU dump file format is designed to be extensible, allowing for future enhancements and additional features without breaking compatibility with existing tools. It is also designed to be streamable, meaning that it can be transported over a network and consumed on the fly. As such, it can be seen as a unidirectional stream of packets. Some portions of the format may describe ways to read back from the GPU, but the format for the readback data is not defined in this document, although it is expected to simply be unstructured binary data. Since the GPU and CPU are both 32-bits, the format is designed to always use 32-bit aligned data types, and as such, all data types in the format are 32-bit aligned. This implies the use of zero-padding where necessary, and a file size that is a multiple of 4 bytes, and all lengths are expressed in 32-bit words. Any string in the format should be zero-padded to ensure 32-bits alignment. Last but not least, the format is little-endian.

## Structure of a GPU Dump File

A GPU dump file is consists of a magic header, followed by a series of packets. The magic header is exactly 16 bytes long. A packet consists of a header followed by a payload. The header of a packet is 4 bytes long, and the payload can vary in size depending on the type of packet. The total size of a packet (header + payload) must also be a multiple of 4 bytes.

### Magic Header

| Byte 0-3 | Byte 4-7 | Byte 8-11 | Byte 12-15 |
|----------|----------|-----------|------------|
| `P S X G`| `P U D U`| `M P v 1` | `r 1 \0 \0`  |

The magic header serves to identify the file format and version. The first 10 bytes (`PSXGPUDUMP`) are a unique identifier for the GPU dump file format, while the last few bytes (`v1r1`) indicate the version of the format. The intended use of the versioning is to allow for future enhancements to the format the following way: the major version is indicated by the number after the `v`, and the minor version is indicated by the number after the `r`. A new major version indicates a breaking change, while a new minor version indicates a non-breaking change. Packets belong to a specific revision of the format, and a tool that reads a GPU dump file should be able to simply ignore packets that it does not understand. And a tool that opens a GPU dump file should be able to read the magic header and determine if it can process the file based on the version. An unknown version must be treated as an error.

### Packet Structure

Each packet consists of a header and a payload. The header is 4 bytes long and contains the following fields:

| Byte 0-2 | Byte 3  |
|----------|---------|
| Length   | Type    |

- **Length**: The length of the payload in 32-bit words (not including the header). This field is 3 bytes long, and are stored on the lowest 3 bytes of the header.
- **Type**: The type of the packet, stored in the highest byte of the header. The type is a single byte that indicates the kind of data contained in the payload. The type is used to determine how to interpret the payload.

A tool that reads a GPU dump file must be able to handle unknown packet types gracefully, skipping over them using the `Length` field, ignoring them without causing an error.

### Packet Types

The following packet types are defined in revision 1 of the GPU dump file format:

| Type | Description                       |
|------|-----------------------------------|
| 0x00 | GPU port 0 data                   |
| 0x01 | GPU port 1 data                   |
| 0x02 | VSync event                       |
| 0x03 | Throw away port 0 data            |
| 0x04 | Readback port 0 data              |
| 0x05 | Trace begin                       |
| 0x06 | GPU version                       |
| 0x10 | Game ID                           |
| 0x11 | Textual video format              |
| 0x12 | Comment                           |

### Description of Packet Types

- **GPU port 0 data (0x00)**: This packet type contains data sent to GPU port 0. The payload consists of a series of 32-bit words representing the data sent to the GPU.
- **GPU port 1 data (0x01)**: Similar to the GPU port 0 data packet, this packet type contains data sent to GPU port 1.
- **VSync event (0x02)**: This packet type indicates a vertical synchronization event. The payload is of variable size, and can be 0, 1, or 2. If non-zero, it contains a single word representing the timestamp of the VSync event, counted in CPU cycles. If the payload is 1, the timestamp will roll over every 2^32 cycles, which would mean that the timestamp is not guaranteed to be unique as it will wrap around roughly every 2 minutes given a 33.8688Mhz clock, whereas a payload of size 2 will roll over every 2^64 cycles, virtually guaranteeing uniqueness as it would take over 17250 years to wrap around. The purpose of this packet is to allow for synchronization of multiple frames animated in a scene.
- **Throw away port 0 data (0x03)**: This packet type indicates that data needs to be read from GPU port 0, but the data is not needed and can be discarded. The payload is a single 32-bit word indicating the number of words to discard. This is useful when the previous command was a VRAM read, in order to restore the GPU state without needing to read back the data.
- **Readback port 0 data (0x04)**: This packet type indicates that data is being read back from GPU port 0. The method of sending this data back is not defined in this document, but it is expected to be unstructured binary data. The payload is a single 32-bit word indicating the number of words to read and send back. This packet should be used when the previous command was a VRAM read.
- **Trace begin (0x05)**: This packet has a size of 0, and indicates the trace actually begins. Any packet type between 0x00 and 0x04 before it have been synthetically generated for the sake of restoring the state of the GPU, and did not actually come from any record.
- **GPU Version (0x06)**: This packet type contains exactly a single word describing the GPU version and VRAM size. A value of 1 means version 1 of the GPU, with 1MB of VRAM attached to it. A value of 2 means version 2 of the GPU, with 1MB of VRAM attached to it. A value of 3 means version 2 of the GPU, with 2MB of VRAM attached to it. All other values are reserved. This packet is only valid before any packet type between 0x00 and 0x05.
- **Game ID (0x10)**: This packet type contains a zero-padded string representing the ID of the game being run, if available. This can be useful for identifying which game the GPU dump file corresponds to. A game ID should be in the format `SLUS-12345` to maintain interoperability with game databases.
- **Textual video format (0x11)**: This packet type contains a zero-padded string representing the video format being used, either "PAL" or "NTSC". Since the GPU is configured using port 1 commands, this packet is only used for informational purposes, similar to the comment packet.
- **Comment (0x12)**: This packet type allows for the inclusion of comments in the GPU dump file. The payload consists of a zero-padded string, which can be used to provide additional context or information about the GPU dump, such as the tool name which was used to create the dump, as well as its version number.

## Usage suggestion

Informational fields, such as the game ID and video format, should be included in the dump file as early as possible, ideally right after the magic header.

Then, in order to provide a properly replayable GPU dump file, any tool that generates a GPU dump file should first start by sending out a manufactured VRAM write command to the GPU, followed by a full VRAM dump, and the state of the GPU can be restored by sending a series of manufactured commands sent to the GPU port 1, setting for example the display area or the display mode.

After that, the tool can start writing data as captured from the GPU.

VSync events should be captured and written to the dump file as they occur in the CPU, as they are important for synchronizing frames. Multiple sequential VSync events mean that more than one frame is being skipped.
